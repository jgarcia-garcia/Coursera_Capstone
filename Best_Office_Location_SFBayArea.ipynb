{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Introduction - Business Problem\n\nAfter finishing our Computer Science studies at Berkeley, we had a great idea and developed a prototype of a pair of augmented reality glasses with full 5G connectivity, bluetooth and a GPS chipset.\nWe avoided to include any camera on it (as Google Glasses) and focusing on social media streaming (as the first version of Google ones) or focusing in enterprises (like the latest one).\nThe reason is that we have identified geolocation as our niche market and we want to focus on it.\n\nIn the process of creating the prototype, we did some proof of concepts with different APIs and we observed that there are lot of functionalities we can offer to the users. \nSome examples for which we implemented proof of concepts:\n\n* Strava integration - Strava is the most used APP by runners around the world and after some exploratory testing with the Strava API, we realized our glasses are able to:\n    + show to the user very useful data while he or she is running, like: velocity, minutes per mile, distance, heart rate, etc...\n    + show them routes previously defined in their user profile or the most popular routes around them\n    + show the user \"kudos\" (cheers) given by their friends in real time\n\n\n* Google Maps integration - Our glasses are able to integrate with the Google Maps API and we can implement very interesting features:\n    + navigate through maps while driving without needing to take the eyes off the road\n    + showing interesting/historical points in a city (very useful for tourists)\n    + showing interesting restaurants/venues around them when required\n\n\n* ESPN Sports live stats integration - Our glasses are able to show live sports stats/scores through the ESPN API\n    + show live scores for matches of the major leagues in the USA (NFL, NBA, NHL and MLB)\n    + show live stats when attending to any event like a Lakers or GSW game\n    + show time differences between different cars in a Nascar race\n\n\n* Disney Parks - As ESPN is part of the Disney emporium, they were impressed by our glasses and ask us to implement as well:\n    + navigation through Disney parks, starting with Magic Kingdom in Orland as a proof of concept\n    + real time stats on waiting times for the park attractions, so the users can optimize their time in the park\n\n\n\nSo, after creating our prototype, we had several appointments with some of the biggest tech companies (including these 3 ones, plus Apple, Microsoft, etc...) in order to show them what our glasses are capable of.\nAlmost all of them were very interested and after a funding round, some of them decided to invest in our project and create a partnership with us in order to develop the final product and implement lots of features in them.\n    \n\nWe have launched a brand new start-up company named *Aviato GeoGlasses Inc* and collected 20 millions from our 4 main investors (Apple, Strava, Google, Disney-ESPN) in exchange of 5% of the shares of the company for each one of them.\n\nNow that we have the necessary funding, our next step is to hire 30 engineers, 10 marketing people and additional 10 support staff people (Human Resources, legal advisors, etc...).\nHowever, before doing that, we need to **look for the best place where our offices will be located** and that is the purpose of this work.\n\nIn order to define where we think is the best place to locate our offices, we have done an brainstorming session and came to the following conclusion:\n\n* we want to locate our offices in the San Francisco Bay Area\n    + there is a lot of personal talent around the bay (graduates from Berkeley, Standford, etc... and engineers working in other tech companies) so we want to attract that people\n    + most of our partners are located around this area\n    + there are lots of potentials new partners (big technological companies and lots of other startups) around the bay\n\nAs we want to accelerate this process and start hiring people and working in the final product as soon as possible, we have decided that we don't want to spend lots of time visiting all the cities around the bay (101 municipalities) searching for the best city/town to locate our offices.\n\nAs we have some expertice working as Data scientists, we are going to use our knowledge to reduce the list of possible locations to only a bunch of locations around the bay and we will only need to visit the list of final candidates.\n\nSo, we have done another brainstorming session in order to clarify which are the requirements we are looking for.\nAs a result of that session, we have come to the following requirements for the location:\n\n* we want that our offices are located around the center of the selected city\n    + want to feel the \"vibe\" of the city \n    + avoid ugly and noisy industrial parks\n    + avoid residential neighbourhoods where there are no venues around (restaurants, gyms, etc...) \n\n\n* we want to locate our office in a town with a reasonable population number\n    + want to avoid very busy/turistic cities with a big population like San Francisco\n    + at same time we want to avoid very little towns with a very small population\n    \n    \n* we believe our employees will be more creative and efficient if they have interesting venues around\n    + we don't believe that working 10 hours without interacting with anybody is a good practice\n    + will look for places with restaurants around, so our employees can go for a lunch and socialize\n    + will look for places with gyms or other sports facilities around\n    + not mandatory, but nice to have, some bars around so our employees can go for a drink after work with other colleagues\n\n\n* we are looking for cities that have some train or bus station around the city center\n    + in order to facilitate our employees to come to the office from other cities around the bay\n\n\n* in case there are too many cities fulfilling the previous requirements, we will use as a filter having other kinds of venues around:\n    + parks, schools, cinemas, shopping malls, etc...\n    + can attract more people to our company and maybe they decided to rent/buy an apartment/house and live in the selected city\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Data adquisition\n\nNow that have defined our problem to solve, we need to start looking for the data required and the data sources that will provide it.\n\n## List of municipalities around SF Bay area\n\nAs a first step, we need a dataset containing the list of municipalities around the San Francisco Bay Area.  \nWe can find that information in the Wikipedia: https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_the_San_Francisco_Bay_Area\n\nAs the information is presented in a table (HTML format), we can use Pandas and Beautiful Soup functionalities in order to retrieve the information.  \nThat table will contain:\n* Name\n* Type (City, Town, etc...)\n* County\n* Population\n* Land Area\n* Incorporation date"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting folium==0.5.0\n  Downloading folium-0.5.0.tar.gz (79 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79 kB 7.9 MB/s  eta 0:00:01\n\u001b[?25hCollecting branca\n  Downloading branca-0.4.2-py3-none-any.whl (24 kB)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from folium==0.5.0) (2.11.2)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from folium==0.5.0) (2.24.0)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from folium==0.5.0) (1.15.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from jinja2->folium==0.5.0) (1.1.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->folium==0.5.0) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->folium==0.5.0) (2020.12.5)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->folium==0.5.0) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->folium==0.5.0) (1.25.9)\nBuilding wheels for collected packages: folium\n  Building wheel for folium (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for folium: filename=folium-0.5.0-py3-none-any.whl size=76240 sha256=1d83bc37a361d9c62cd1bf1c21b7fa1a3a6a2b250919a98bd09ef3c267a2dfbe\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/b2/2f/2c/109e446b990d663ea5ce9b078b5e7c1a9c45cca91f377080f8\nSuccessfully built folium\nInstalling collected packages: branca, folium\nSuccessfully installed branca-0.4.2 folium-0.5.0\n"
                }
            ],
            "source": "!pip install folium==0.5.0\nimport folium # plotting library"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "#Download the html from the URL and convert into a BeautifulSoup object\nurl='https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_the_San_Francisco_Bay_Area'\nhtml_data  = requests.get(url).text \nsoup_object = BeautifulSoup(html_data,\"html5lib\")  # create a soup object using the variable 'html_data'"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "soup_object = BeautifulSoup(html_data,\"html5lib\")  # create a soup object using the variable 'html_data'\n#Extract the tables/table\nwiki_tables = soup_object.find_all('table')\n\n#Use pandas to transform the table into a dataframe\nwiki_df = pd.read_html(str(wiki_tables[1]),flavor='bs4')[0]"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Type</th>\n      <th>County</th>\n      <th>Population</th>\n      <th>Land area - sq mi</th>\n      <th>Land area - km2</th>\n      <th>Incorporated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alameda</td>\n      <td>City</td>\n      <td>Alameda</td>\n      <td>73812</td>\n      <td>10.61</td>\n      <td>27.5</td>\n      <td>April 19, 1854</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albany</td>\n      <td>City</td>\n      <td>Alameda</td>\n      <td>18539</td>\n      <td>1.79</td>\n      <td>4.6</td>\n      <td>September 22, 1908</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>American Canyon</td>\n      <td>City</td>\n      <td>Napa</td>\n      <td>19454</td>\n      <td>4.84</td>\n      <td>12.5</td>\n      <td>January 1, 1992</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antioch</td>\n      <td>City</td>\n      <td>Contra Costa</td>\n      <td>102372</td>\n      <td>28.35</td>\n      <td>73.4</td>\n      <td>February 6, 1872</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Atherton</td>\n      <td>Town</td>\n      <td>San Mateo</td>\n      <td>6914</td>\n      <td>5.02</td>\n      <td>13.0</td>\n      <td>September 12, 1923</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "              Name  Type        County  Population  Land area - sq mi  \\\n0          Alameda  City       Alameda       73812              10.61   \n1           Albany  City       Alameda       18539               1.79   \n2  American Canyon  City          Napa       19454               4.84   \n3          Antioch  City  Contra Costa      102372              28.35   \n4         Atherton  Town     San Mateo        6914               5.02   \n\n   Land area - km2        Incorporated  \n0             27.5      April 19, 1854  \n1              4.6  September 22, 1908  \n2             12.5     January 1, 1992  \n3             73.4    February 6, 1872  \n4             13.0  September 12, 1923  "
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#Need to rename the headers, in the original table there are 2 levels\nwiki_df.columns = ['Name','Type','County','Population','Land area - sq mi','Land area - km2','Incorporated']\nwiki_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we are only interested in the name of the city/town and the population, we are going to remove the rest of the columns (except for county column, we will see later that we need that one as \nwell).\n\nAdditionally, we are going to apply the \"reasonable population\" filter that we defined as a requisite in our analysis.\nIn concrete, we are are going to remove from our data:\n* Big cities, with population bigger than 120000 citizens\n* Small towns, with population less than 20000 citizens"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "wiki_df.drop(['Type','Land area - sq mi','Land area - km2','Incorporated'],axis=1,inplace=True)"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>County</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alameda</td>\n      <td>Alameda</td>\n      <td>73812</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albany</td>\n      <td>Alameda</td>\n      <td>18539</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>American Canyon</td>\n      <td>Napa</td>\n      <td>19454</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antioch</td>\n      <td>Contra Costa</td>\n      <td>102372</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Atherton</td>\n      <td>San Mateo</td>\n      <td>6914</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "              Name        County  Population\n0          Alameda       Alameda       73812\n1           Albany       Alameda       18539\n2  American Canyon          Napa       19454\n3          Antioch  Contra Costa      102372\n4         Atherton     San Mateo        6914"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "wiki_df.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "wiki_df_final = wiki_df[(wiki_df['Population']>20000) & (wiki_df['Population']<150000)]\nwiki_df_final.reset_index(inplace=True)"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "wiki_df_final.drop(['index'],axis=1,inplace=True)"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(60, 3)\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>County</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alameda</td>\n      <td>Alameda</td>\n      <td>73812</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Antioch</td>\n      <td>Contra Costa</td>\n      <td>102372</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Belmont</td>\n      <td>San Mateo</td>\n      <td>25835</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Benicia</td>\n      <td>Solano</td>\n      <td>26997</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Berkeley</td>\n      <td>Alameda</td>\n      <td>112580</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "       Name        County  Population\n0   Alameda       Alameda       73812\n1   Antioch  Contra Costa      102372\n2   Belmont     San Mateo       25835\n3   Benicia        Solano       26997\n4  Berkeley       Alameda      112580"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "print(wiki_df_final.shape)\nwiki_df_final.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we can observe, with the population filter, we have reduced the number of municipalities that could fit our requirements from the original 101 municipalities into 60.\n\nNow we will continue the analysis in order to refine the list and reduce the number of potential candidates.\n\n## Longitude and Latitude for each city\n\nWe have decided to use the FourSquare API as the service that will provide the list of venues around the city centers of the selected cities.\nIn order to be able to use that API, we first need to retrieve the coordinates for each city center.\n\nWe are going to use the Geopy-Nominatim Python package in order to get those longitudes and latitudes required.\nWe will loop through the cities and retrieve the coordinates for each one of them."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install geopy\nfrom geopy.geocoders import Nominatim # module to convert an address into latitude and longitude values"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "geolocator = Nominatim(user_agent=\"SFBay_Explorer\")\ncoordinates_df = pd.DataFrame(columns=['Name', 'County','Latitude', 'Longitude'])\n\nfor name,county in zip(wiki_df_final['Name'], wiki_df_final['County']):\n    #In order to create the address, add state name (California) to the town name\n    address = name + ' ,California'\n    location = geolocator.geocode(address)\n    latitude = location.latitude\n    longitude = location.longitude\n    coordinates_df = coordinates_df.append({'Name': name, 'County':county, 'Latitude' : latitude, 'Longitude': longitude}, ignore_index=True)\n    #print('The geograpical coordinate of {} are {}, {}.'.format(name, latitude, longitude))\n\ncoordinates_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we want to make sure that the coordinates for each town corresponds to the city center, we are going to print in the San Francisco Bay map each city center location with a marker.\nIn order to do that, we will use Folium package for Python."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Create the map centered in the San Francisco Bay\nbay_address = 'San Francisco Bay, California'\nbay_location = geolocator.geocode(bay_address)\nbay_latitude = bay_location.latitude\nbay_longitude = bay_location.longitude\n#print('The geograpical coordinate of {} are {}, {}.'.format(bay_address, bay_latitude, bay_longitude))\nSFBay_map = folium.Map(location=[bay_latitude, bay_longitude], zoom_start=9)\n\n#Add markers for each location\nfor lat, lng, name, county in zip(coordinates_df['Latitude'], coordinates_df['Longitude'], coordinates_df['Name'],coordinates_df['County']):\n    label = '{}, {}'.format(name, county)\n    label = folium.Popup(label, parse_html=True)\n    \n    #Print incorrect markers in red\n    if (name in ['Alameda','Santa Clara', 'San Mateo']):\n        color='red'\n        fill_color='#d97652'\n    else:\n        color='blue'\n        fill_color='#3186cc'\n    \n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color=color,\n        fill=True,\n        fill_color=fill_color,\n        fill_opacity=0.7,\n        parse_html=False).add_to(SFBay_map)  \n\nSFBay_map"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can observe in the map that there are a few points that are located in the supposed center of the city. We have marked them in red color.\n\nThose cities are Alameda, Santa Clara and San Mateo.\nThe reason for these anomalies is that the county they are located have the same name and Nominatim library is returning the coordinates of the geographical county center. \n\nWe are going to fix the coordinates for these 3 anomalies. As the number of anomalies is quite small compared with the total population of cities, we are going to manually check the city centers looking in Google Maps and use those adresses to look again for the latitude and longitude of those cities and update the dataframe with the correct values."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Looking for the coordinates of the 3 anomalies\nad1 = 'Main St, Alameda, California'\nad2 = 'Triton Museum of Art, Santa Clara, California'\nad3 = 'San Mateo City Hall, San Mateo, California'\n\nloc1 = geolocator.geocode(ad1)\nloc2 = geolocator.geocode(ad2)\nloc3 = geolocator.geocode(ad3)\n\nlat1 = loc1.latitude\nlat2 = loc2.latitude\nlat3 = loc3.latitude\n\nlong1 = loc1.longitude\nlong2 = loc2.longitude\nlong3 = loc3.longitude\n\n#print('The geograpical coordinate for {} of are {}, {}.'.format(ad1, lat1, long1))\n#print('The geograpical coordinate for {} of are {}, {}.'.format(ad2, lat2, long2))\n#print('The geograpical coordinate for {} of are {}, {}.'.format(ad3, lat3, long3))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Adding the coordinates into our dataframe\ncoordinates_df[coordinates_df['Name']=='Alameda'] = ['Alameda','Alameda',lat1,long1]\ncoordinates_df[coordinates_df['Name']=='Santa Clara'] = ['Santa Clara','Santa Clara',lat2,long2]\ncoordinates_df[coordinates_df['Name']=='San Mateo'] = ['San Mateo','San Mateo',lat3,long3]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Venues for each city - FourSquare API\n\nFinally, once we have the coordinates for each location, we are able to invoke the FourSquare API in order to get the venues around.\n\nWe don't want that our employees have to ride an Uber or spend 1 hour walking, for example, if they want to have lunch in a restaurant or want to go to the gym during the lunch break.\nSo, for that reason, we are going to limit the search radius to 500 meters from the city center.\n\nWe are going to create a dataframe containing all the venues retrieved for the 60 cities in our list that are within those 500 meters from city centers.\nIn order to do that, we will loop through all the neighborhoods, doing a request to FourSquare for getting the venues for each one of them and including all the venues in a unique dataframe."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def getNearbyVenues(names, latitudes, longitudes, radius):\n    \n    venues_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        #print(name)\n            \n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['name'], \n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['City', \n                  'City Latitude', \n                  'City Longitude', \n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_venues)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "SFBay_venues = getNearbyVenues(names=coordinates_df['Name'],\n                                   latitudes=coordinates_df['Latitude'],\n                                   longitudes=coordinates_df['Longitude'],\n                                   radius=radius\n                                  )"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(SFBay_venues.shape)\nSFBay_venues.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we can observe in the dataframe, we have retrieved 2077 different venues for the 60 possible city locations for our offices.\n\nIn the next section, we will start doing our data transformation and analysis in order to find the best locations within these 60 cities."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Methodology\n\nIn order to do the final selection of city candidates for hosting our offices, we are going to do an statistical analysis focused on the main venue categories we are interested in.\nWe will analyze the histogram and the median, interquantile range through a boxplot for all the categories we are specially interested in.\nFinally, based on this analysis, will select the minimum number of venues for each category that we expect in our selected city and filter the list of candidates based on that criteria.\n\nAs discussed in previous sections, we will be interested in analyzing the number of venues for each location within the following categories:\n\n* food places like restaurants of any kind (italian, mexican, japanese, etc...) and more casual food places (hot dogs, pizza places, etc...)\n\n* sport venues of any kind (gyms, martial arts schools, athletics tracks, etc...)\n\n* public transport stations (bus, metro, train...)\n\n* afterwork places like bars, pubs, gastropubs, etc...\n\n* other interesting venues like entertainment places (cinemas, theaters...), shops/stores, soft drinks (coffee, juice), etc...\n\nWe are going to start looking for the number of unique venue categories that we can observe in our venues data previously adquired through FourSquare.\nAs we can observe, there are 266 different categories. \nIn fact, if we observe the first 10 of them, we can deduce that categories are quite atomized and we will need to group them into more general categories."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Look for unique venue categories\nvenue_cats = pd.DataFrame(columns=['Venue Category'])\nvenue_cats['Venue Category'] = SFBay_venues['Venue Category'].unique()\nvenue_cats.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#There are 266 different categories\nvenue_cats.head(10)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Aggregated Category Classification\n\nIn order to categorize the venues in our list into aggregated categories, we have to follow the next steps:\n\n* define the list of aggregated categories (bar, beauty, casual food, entertainment, groceries, health services, hotel, laundry, legal & financial, public transport, restaurants, soft drinks, sports, stores and others)\n\n* create the mapping between the venue categories obtained from FourSquare and our new aggregated categories\n\n* apply that mapping and add a new column to our data dataframe with the aggregated category for each venue\n\nIf we try to automatize that task, we will arrive to the conclusion that is too complex. \nMost of the FourSquare categories that will be included in the same aggregated have a quite different name. \nHence is not possible to look for a common pattern that will allow us to automatize this task.\n\nSo, as a manual solution, we have created a CSV file with the mapping and loaded it as a dataframe."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Then we have joined it with our venues list, adding a new column in there that contains the aggregated category as well"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Create dataframe joining venues with category map\nbay_venues_cat_df = SFBay_venues\nbay_venues_cat_df = bay_venues_cat_df.join(category_map.set_index('Venue Category'),on='Venue Category')\nbay_venues_cat_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Category grouping\n\nOur next task will consist in counting the number of venues for each different combination of aggregated category and location.\n\nIn order to fulfill that task, we will first to apply one hot encodding as an auxiliar step that will help us.\nThat means that a new dataframe is created where each column corresponds to a different unique category and there is a row for each venue.\nThe value of cell will be 0, if the venue is not included the corresponding category, or 1 if it is included.\n\n\nWe will apply one hot encoding for the *Aggregated Category* column and add the *City* column again as well."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Use one hot encoding \nbay_onehot = pd.get_dummies(bay_venues_cat_df[['Aggregated Category']], prefix=\"\", prefix_sep=\"\")\n\n#Add city column back to dataframe\nbay_onehot.insert(0, 'City', bay_venues_cat_df['City'], True)\n\n#print(bay_onehot.shape)\nbay_onehot.head(5)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Once the one hot coding auxiliar dataframe is ready, we are going to group by city and count the number of places of each type per location.\nThis will be our final dataframe that will be very helpful for the statistical analysis that we are going to do in the next steps."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Use group_by function to count the number of venues for each category for each location\nbay_grouped = bay_onehot.groupby('City').sum().reset_index()\nbay_grouped.head(10)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Analyzing Food Places\n\nAs the first step in our statistical analysis, we are going to analyze the statistics for the number of food places in each location.\n\nWe have divided the food places into 2 different categories: restaurants and casual food, so we are going to analyze both categories at same time.\n\nIf we plot the histograms for both categories, we can observe that the distribution doesn't follow any specific pattern like a gaussian distribution, for example."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Histogram of restaurants per city\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\nfig.suptitle('Food Places Per City Histograms',fontsize=14, fontweight='bold')\n\nbins0 = np.arange(0, bay_grouped['Restaurant'].max() + 1.5) - 0.5\nsns.distplot(bay_grouped['Restaurant'], ax=axes[0],hist=True, kde=False, \n             bins=bins0, color = 'darkblue', \n             hist_kws={'edgecolor':'black'})\n\naxes[0].set_title('Restaurants per City')\naxes[0].set_xlabel('Number of restaurants')\naxes[0].set_ylabel('Number of cities')\naxes[0].set_xticks(np.arange(0, 40, step=4)) \n\nbins1 = np.arange(0, bay_grouped['Casual Food'].max() + 1.5) - 0.5\nsns.distplot(bay_grouped['Casual Food'], ax=axes[1], hist=True, kde=False, \n             bins=bins1, color = 'yellow', \n             hist_kws={'edgecolor':'black'})\n\naxes[1].set_title('Casual Food Places per City')\naxes[1].set_xlabel('Number of casual food places')\naxes[1].set_xticks(np.arange(0, 18, step=2)) "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The distribution is quite random, hence we will show the median and interquantile percentages as well in a boxplot in order to help us to make our decission of which should be the minimum number of food places required for our selected candidate cities."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Box plots\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\nfig.suptitle('Food Places Per City BoxPlots',fontsize=14, fontweight='bold')\n\nsns.boxplot(y=bay_grouped['Restaurant'],ax=axes[0], color = 'darkblue',saturation=0.4)\nsns.boxplot(y=bay_grouped['Casual Food'],ax=axes[1], color = 'yellow', saturation=0.4)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "If we analyze those boxplots, we can observe that the median for each category is:\n\n* 9 for restaurants\n* 7 for casual food places\n\nWe have decided to use those median values as the minimum requirements that will be introduced in our filter for the candidate cities.\nThe reasoning behind this decission is that:\n\n* 9 different restaurants is enough for our employees doesn't need to repeat restaurants at least in couple of weeks, even if they decide to go have lunch everyday or they decide\n\n* 7 different casual food places is enough as well for our employees if they decide to have a quick lunch or dinner and have enough different choices\n\n* those filters are not very restrictive, so we still expect that 50% of our cities will pass the food places cut\n\n## Analyzing Sport Venues\n\nWe are going to do an analogous analysis for the number of sport venues.\n\nIn this case, we are going to plot directly the histogram and boxplot together in order to help us to find the filter to apply."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Sports venues\n# Histogram of restaurants per city\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\nfig.suptitle('Sport Venues per City',fontsize=14, fontweight='bold')\n\nbins = np.arange(0, bay_grouped['Sports'].max() + 1.5) - 0.5\nsns.distplot(bay_grouped['Sports'], ax=axes[0],hist=True, kde=False, \n             bins=bins, color = 'green', \n             hist_kws={'edgecolor':'black'})\n\naxes[0].set_xlabel('Number of sports venues')\naxes[0].set_ylabel('Number of cities')\n\nsns.boxplot(y=bay_grouped['Sports'],ax=axes[1], color = 'green',saturation=0.4)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we can observe, the median of sport venues for our list of cities is 2.\n\nHowever, we think that is a very low number for our requirements, so we are going to set the filter of minimum number of sport venues to 3.\n\nThe reasoning behind this decission is that taking into account that the 75% percentile is 3, we still expect that more than 25% of our pre-selected locations will pass this filter.\n\n\n## Analyzing Public Transport Stations\n\nIn our next statistical analysis, we are going to take a look to the number of public transport stations located in the center of each of our pre-selected cities.\n\nIn order to help us with out analysis, we are going to plot the histogram and a boxplot as we already did with other previous categories in our analysis."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Public transport\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\nfig.suptitle('Public Transport Stations per City',fontsize=14, fontweight='bold')\n\nbins = np.arange(0, bay_grouped['Public Transport'].max() + 1.5) - 0.5\nsns.distplot(bay_grouped['Public Transport'], ax=axes[0],hist=True, kde=False, \n             bins=bins, color = 'red', \n             hist_kws={'edgecolor':'black'})\n\naxes[0].set_xlabel('Public transport stations')\naxes[0].set_ylabel('Number of cities')\naxes[0].set_xticks(np.arange(0, 3, step=1)) \n\nsns.boxplot(y=bay_grouped['Public Transport'],ax=axes[1], color = 'red',saturation=0.4)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we can observe, as per the data retrieved from FourSquare, most of the cities didn't have any public transport station in the city center.\n\nWe know that there are several bus and metro lines across the San Francisco Bay, so it seems to be that public stations are not well reflected in the FourSquare database.\n\nHence, as we cannot really trust the FS data for this category, we are not going to include any specific filter, for selecting our candidates cities, related to public transport.\n\n## Analyzing Afterwork Drinks Venues\n\nIn previous sections, we defined as a not mandatory but nice to have requirement, the existence of venues where our employees can go afterwork for a drink and socialize with other colleages.\n\nSo, we are going to do the same statistical analysis that we did with previous categories and select an appropiate mininum number of drinks places for our filter of candidate cities."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Bars\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\nfig.suptitle('Bars/Pubs per City',fontsize=14, fontweight='bold')\n\nbins = np.arange(0, bay_grouped['Bar'].max() + 1.5) - 0.5\nsns.distplot(bay_grouped['Bar'], ax=axes[0],hist=True, kde=False, \n             bins=bins, color = 'violet', \n             hist_kws={'edgecolor':'black'})\n\naxes[0].set_xlabel('Number of bars/pubs')\naxes[0].set_ylabel('Number of cities')\naxes[0].set_xticks(np.arange(0, 18, step=2)) \n\nsns.boxplot(y=bay_grouped['Bar'],ax=axes[1], color = 'violet',saturation=0.4)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we can observe in the histogram and in the boxplot, the median for number of bars/pubs in the pre-selected cities is 1 but there are still quite a lot of cities with 2 or more bars/pubs.\n\nHence, we have decided that we will discard cities that doesn't have 2 or more drinks venues allocated in there.\n\n## Candidate Cities and Heatmap\n\nIn the previous steps we have defined the filters that we will apply in order to select our final candidate cities.\n\nAs a recap, those filters are:\n\n* at least 9 restaurants allocated in the city center\n\n* at least 7 casual food places allocated in the city center\n\n* at least 3 sports venues allocated in the city center\n\n* at least 2 afterwork drinks spots (bars/pubs) allocated in the city center\n\nSo, if we apply those filters into the pre-selection of 60 cities that matched our population requirement, we will get the following selection of cities:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Use the requirements to filter the cities and do a first selection\n#Requirements are at least: 9 restaurants, 7 casual food places, 3 sport venues, 2 bars\nbay_filtered = bay_grouped[(bay_grouped['Restaurant']>=9)&(bay_grouped['Casual Food']>=7)&(bay_grouped['Sports']>=3)&(bay_grouped['Bar']>=2)]\nprint(bay_filtered.shape)\nbay_filtered"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we can observe, there are still 11 candidate cities matching our minimum requirements.\n\nThe goal of this exercise was to reduce the number of cities that we will need to visit in order to decide what would be the best place for locating our office and we consider that 11 cities is still a big number of cities to visit and we want to save our time.\n\nSo, as a final step, we are going to select the best cities between those 11 candidates.\n\nIn order to do that, we have decided that we are going to plot a heatmap showing the number of venues for each combination of one of the most interesting categories and candidate city.\nThe heatmap will help us to compare the number of venues between cities and will be very useful to decide the finalist cities."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#We are going to plot a heat map and decide the finalists between the 9 filtered cities\n#For that, we are going to take into account as well: Entertainment, Soft Drinks places and Stores\n\n#1st - Remove non-interesting categories from our dataframe\nbay_filter_places = bay_filtered[['City','Bar','Casual Food','Entertainment','Public Transport','Restaurant','Soft Drinks','Sports','Store']]\n\n#Use the city as index\nbay_heat = bay_filter_places.set_index('City')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Plot the heatmap\nsns.heatmap(bay_heat,annot = True, fmt=\"d\", linewidths=0, cmap = 'coolwarm', xticklabels = True)\nplt.ylabel('City')\nplt.xlabel('Venue category')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This absolute heatmap is quite interesting to observe the absolute number of venues for each city but in order to compare different categories is not the best option.\n\nFor example, as the number of restaurantes for each city is always much higher than sports venues, sports venues are printed in quite similar shades of blue, hence difficult to compare cities just taking a look to the heatmap.\n\nSo, we are going to normalize each category dividing the number of venues per category/city by the maximum number of venues in each category. \nThat way we will have always a value between 0 and 1 and will be very useful for comparing our different candidate cities."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Need to normalize the heatmap dataframe for better visualization purposes\n#Will divide each column by max value so we get a value between 0 and 1\nnormalized_bay_heat=(bay_heat-bay_heat.min())/(bay_heat.max()-bay_heat.min())\n\nsns.heatmap(normalized_bay_heat,annot = True, linewidths=0, cmap = 'Blues', xticklabels = True)\nplt.xlabel('Venue category')\nplt.ylabel('City')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In the next section, we will analyze this heatmap and will finally get to the conclusion of which will be our final selection of cities to visit.\n\n# Results\n\nFinally, we are going to decide which cities of these 11 finalists that fulfilled our previous requirements are the best options for locating our office.\nIf we study the pros and cons of each one of those:\n\n* Campell - Discarded\n    + Big number of casual food and entertainment venues\n    + However, is one of the finalists with less restaurants, sport venues, bars, etc...\n \n \n* Danville - Discarded\n    + It's one of the cities with more casual food and soft drinks places\n    + However, is one of the finalists with less restaurants, sport venues, etc...\n\n\n* Hayward - Discarded\n    + It's the city with less interesting venues within the finalists\n\n\n* Livermore - Discarded\n    + The one with more bars and a big number of casual food places. There is also a public transport station\n    + However, the numbers of restaurants is low compare with the rest of finalists and the number of other venues like stores or entertainment venues is quite low as well\n\n\n* Menlo park - Discarded\n    + It's the second city with less interesting venues within the finalists\n\n\n* Napa - Selected\n    + High number of bar, casual food, restaurants and entertainment venues. In the average for sports and soft drinks\n\n\n* Palo Alto - Selected\n    + High number of casual food, entertainment, soft drinks and sport venues. Additionally, there is at least one public transport station\n    + Low number of bars and restaurants compared to other candidate cities\n\n\n* Pleasant Hill - Discarded\n    + High number of casual food, soft drinks and stores (it seems to be that there is a shopping mall close to the city center)\n    + However, it has a very low number of bars, restaurants and sports venues compared to other cities\n\n\n* Redwood City - Selected\n    + High number of casual food, restaurants, bars, soft drinks and entertainment places compared to the rest of candidates\n    + Only 3 sports venues\n\n\n* San Carlos - Discarded\n    + High number of casual food and restaurants\n    + However, for the rest of categories, the number is quite low compared to the other candidates\n\n\n* Walnut Creek - Selected\n    + The one with highest number of casual food, restaurants and sports venues\n    + Acceptable number of bars and entertainment places\n\nSo, our finalist cities will be 4: **Napa, Palo Alto, Redwood City and Walnut Creek**. \nWe will show them in the San Francisco Bay map."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "SFBay_map = folium.Map(location=[bay_latitude, bay_longitude], zoom_start=9)\n\n#Add markers for each location\nfor lat, lng, name, county in zip(coordinates_df['Latitude'], coordinates_df['Longitude'], coordinates_df['Name'],coordinates_df['County']):\n    label = '{}, {}'.format(name, county)\n    label = folium.Popup(label, parse_html=True)\n    \n    #Print final markers in red\n    if (name in ['Napa','Palo Alto','Redwood City','Walnut Creek']):\n        color='red'\n        fill_color='#d97652'\n        rad=8\n    else:\n        color='blue'\n        fill_color='#3186cc'\n        rad=5\n    \n    folium.CircleMarker(\n        [lat, lng],\n        radius=rad,\n        popup=label,\n        color=color,\n        fill=True,\n        fill_color=fill_color,\n        fill_opacity=0.7,\n        parse_html=False).add_to(SFBay_map)  \n\nSFBay_map"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In fact, if we observe the map, Napa is quite far away from the rest of the cities in the Bay, so we can discard that as well if we are looking for a good comunicated city.\n\nAs a result of this statistical analysis and the different requisites we have used for filtering the candidate cities, we finally only need to visit 3 cities (Palo Alto, Redwood City and Walnut Creek) in order to decide which is the best location for our new offices.\n\nThis will save us a huge amount of time, compared to having to visit the 101 municipalities that are part of San Francisco Bay area."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Discussion\n\nThere are some points that we could improve in our analysis in future iterations:\n\n* The number of public transport stations retrieved from FourSquare doesn't seem to be completely accurate as for most of the cities FourSquare has not returned any.\n\n    + In order to improve this, for example, we could do some webscrapping in the Bay Area Rapid Transit website (https://www.bart.gov/stations) and major bus agencies, ferries, etc...\n\n\n* The office rental prices for each location was not important in our requirements as we prefer that our employees like the office location instead of saving some money with a cheaper rent. However, it could be interesting adding that information in order to help us to take our final decission.\n    \n    + We could look for a rental price report for the SF Bay in the internet and use webscrapping to get the data\n\n    + We could add the average rental price per square meter as a new column in our venues dataframe and use that information in our final heatmap in order to help us to take a decission\n    \n    + For example, in case of having 2 cities with a similar number of venues, that are both good candidates, we can select the one with lowest rental prices\n    \n    \n* Executing our automated scripts several times, most of the time we get 11 candidate cities that are passing our filters, but there are few times where there are only 9 or 10.\n\n    + We have noticed that the FourSquare API is not always returning the same number of venues. It's always around 2000 venues, but there are differences of 20 venues above or below that number\n    \n    + As a consecuence, there are a couple of cities (Campbell and Menlo Park) that can barely meet the requirements, that sometimes are not included in the cut\n    \n    + In order to fix this, when we retrieve all the venues, it could be interesting to save that dataframe into a CSV\n    \n "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Conclusion"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The goal of this analysis was to reduce the number of candidate cities for our new office locations from 101 possible candidates (mucipalities inside the San Francisco Bay Area) into only 3 or 4 final candidates. \n\nThat will save us a lot of time and allow us to start our business as soon as possible.\n\nIn order to achieve our goal, our first step was to define our requirements:\n\n* medium population city, between 20000 and 120000 citizens\n\n* our office must be located within 500 meters of the city center\n\n* to filter based on minimum number of venues of each desired type: restaurants, sports venues, bars, public transport stations\n\n* other number of venues for other interesting categories: entertainment places, stores and shops, etc...\n\nThe next step consisted on adquiring our data required and we did that from 3 sources:\n\n* obtained list of municipalities and population from Wikipedia using webscrapping\n\n* obtained coordinadtes of the city center for each one through the Nominatim API for Python\n\n* obtained the list of venues within each city center though requests to the FourSquare web API\n\nOnce the necessary data was retrieved, it was necessary to do some transformations: grouping the 200+ different venue categories into aggregated venue categories and count the number of venues of each type for each location.\n\nAfter our transformation was ready, we did our statistical analysis of the number of venues per each combination of cities and venue categories in order to get our final filters:\n\n* the minimum number of restaurants required is 9 and casual food places is 7\n\n* a minimum number of 3 sports venues allocated in the city center\n\n* at least 2 bars required\n\nAfter applying those filters, there were still 11 city candidates remaining, so as a final step, we plotted a heat map in order to compare the number of venues for the most interesting categories (restaurants, sports venues, bars, shops, entertainment places, public transport stations...) between those different final candidates.\n\nAs a final step, after analyzing the heat map, we decided that the 4 candidates that best fit our requirements are: Napa, Palo Alto, Redwood City and Walnut Creek.\nHowever, after printing in a map the location of those 4 cities, we decided to remove Napa from the list as the location was far away from the rest of the cities in the Bay.\n\nHence, we have been able to reduce the initial 101 potential locations into a final selection of 3 cities. That will make our task of visiting and deciding the final location of our offices much more easier and will save us lots of time."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}